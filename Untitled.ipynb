{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48280772-fdd2-4ef9-a79a-09441ab6c817",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from models import create_model\n",
    "\n",
    "import os\n",
    "from options.train_options import TrainOptions\n",
    "from models import create_model\n",
    "from util.visualizer import save_images\n",
    "from util import html\n",
    "\n",
    "import string\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "from matplotlib import cm\n",
    "\n",
    "from util import util\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5d8ac15-27d8-4843-ae60-6020253e6545",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'A': 23.0,\n",
      " 'B': 23.0,\n",
      " 'ab_max': 110.0,\n",
      " 'ab_norm': 110.0,\n",
      " 'ab_quant': 10.0,\n",
      " 'aspect_ratio': 1.0,\n",
      " 'avg_loss_alpha': 0.986,\n",
      " 'batch_size': 1,\n",
      " 'beta1': 0.9,\n",
      " 'checkpoints_dir': '/media/john/New Volume/DLProject/checkpoints',\n",
      " 'classification': False,\n",
      " 'dataroot': './dataset/ilsvrc2012/val/',\n",
      " 'dataset_mode': 'aligned',\n",
      " 'display_freq': 10000,\n",
      " 'display_id': -1,\n",
      " 'display_ncols': 5,\n",
      " 'display_port': 8097,\n",
      " 'display_server': 'http://localhost',\n",
      " 'display_winsize': 256,\n",
      " 'epoch_count': 0,\n",
      " 'fineSize': 176,\n",
      " 'gpu_ids': [0],\n",
      " 'half': False,\n",
      " 'how_many': 200,\n",
      " 'init_type': 'normal',\n",
      " 'input_nc': 1,\n",
      " 'isTrain': True,\n",
      " 'l_cent': 50.0,\n",
      " 'l_norm': 100.0,\n",
      " 'lambda_A': 1.0,\n",
      " 'lambda_B': 1.0,\n",
      " 'lambda_GAN': 0.0,\n",
      " 'lambda_identity': 0.5,\n",
      " 'loadSize': 256,\n",
      " 'load_model': True,\n",
      " 'lr': 0.0001,\n",
      " 'lr_decay_iters': 50,\n",
      " 'lr_policy': 'lambda',\n",
      " 'mask_cent': 0.5,\n",
      " 'max_dataset_size': inf,\n",
      " 'model': 'pix2pix',\n",
      " 'n_layers_D': 3,\n",
      " 'name': 'siggraph_class_small',\n",
      " 'ndf': 64,\n",
      " 'ngf': 64,\n",
      " 'niter': 100,\n",
      " 'niter_decay': 100,\n",
      " 'no_dropout': False,\n",
      " 'no_flip': False,\n",
      " 'no_html': False,\n",
      " 'no_lsgan': False,\n",
      " 'norm': 'batch',\n",
      " 'num_threads': 1,\n",
      " 'output_nc': 2,\n",
      " 'phase': 'val',\n",
      " 'pool_size': 50,\n",
      " 'print_freq': 200,\n",
      " 'resize_or_crop': 'resize_and_crop',\n",
      " 'results_dir': './results/',\n",
      " 'sample_Ps': [1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
      " 'sample_p': 1.0,\n",
      " 'save_epoch_freq': 1,\n",
      " 'save_latest_freq': 5000,\n",
      " 'serial_batches': True,\n",
      " 'suffix': '',\n",
      " 'update_html_freq': 10000,\n",
      " 'verbose': False,\n",
      " 'which_direction': 'AtoB',\n",
      " 'which_epoch': 'latest',\n",
      " 'which_model_netD': 'basic',\n",
      " 'which_model_netG': 'siggraph'}\n"
     ]
    }
   ],
   "source": [
    "# Load stylized options (pickled from script run to avoid argparse)\n",
    "with open('opt.stylized_small.pkl', 'rb') as f:\n",
    "    opt = pickle.load(f)\n",
    "\n",
    "# opt.gpu_ids = []\n",
    "\n",
    "pprint(opt.__dict__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4efec15-2cb5-48fa-820c-be4b498b62d5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Other variables and config options\n",
    "sample_ps = [1., .125, .03125]\n",
    "to_visualize = ['gray', 'hint', 'hint_ab', 'fake_entr', 'real', 'fake_reg', 'real_ab', 'fake_ab_reg', ]\n",
    "S = len(sample_ps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48fe1def-a558-41dd-a725-55f73d1a652e",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "dataset = torchvision.datasets.ImageFolder(\n",
    "    opt.dataroot,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.Resize((opt.loadSize, opt.loadSize)),\n",
    "        transforms.ToTensor()])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b04482ce-d7a8-4c12-a684-a2397a8b77d0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create dataset loader\n",
    "dataset_loader = torch.utils.data.DataLoader(\n",
    "    dataset, \n",
    "    batch_size=opt.batch_size, \n",
    "    shuffle=not opt.serial_batches\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24d4308-9e97-46d5-856d-bc026c2a16e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7dea10-403d-4587-9678-8cfa82762bfa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7143eee-553e-4e11-970c-71e452e4d4de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ed5760-740f-4e05-8598-53d65def6d3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f494550-3e5e-4097-b8c2-cc568e15e89c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719babc2-a678-4449-876e-78a27d7664e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86c74e4-2e90-4d1e-b609-10b06762f80b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "960430f2-9988-4f74-903d-50cc12c7d673",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialize network with normal\n",
      "model [Pix2PixModel] was created\n",
      "loading the model from /media/john/New Volume/DLProject/checkpoints/siggraph_class_small/latest_net_G.pth\n",
      "---------- Networks initialized -------------\n",
      "[Network G] Total number of parameters : 34.187 M\n",
      "-----------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Load model\n",
    "model = create_model(opt)\n",
    "model.setup(opt)\n",
    "model.eval()\n",
    "\n",
    "# model.netG = model.netG.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a274bcb0-40b3-4f25-a39f-8d64cbb749bb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "activation = {}\n",
    "\n",
    "# Wrapper function to create \"hooks\" for layer activations\n",
    "def get_activation(name):\n",
    "    \n",
    "    # Inner \"hook\" function\n",
    "    def hook(model, input, output):\n",
    "        if name in activation:\n",
    "            activation[name].append(output.detach())\n",
    "        else:\n",
    "            activation[name] = [output.detach()]\n",
    "        \n",
    "    return hook\n",
    "\n",
    "model1_hook = model.netG.module.model1.register_forward_hook(get_activation('model1'))\n",
    "model2_hook = model.netG.module.model2.register_forward_hook(get_activation('model2'))\n",
    "model3_hook = model.netG.module.model3.register_forward_hook(get_activation('model3'))\n",
    "model4_hook = model.netG.module.model4.register_forward_hook(get_activation('model4'))\n",
    "model5_hook = model.netG.module.model5.register_forward_hook(get_activation('model5'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e77bd519-0d84-426e-a530-5ace706bc1eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i, data_raw in enumerate(dataset_loader):\n",
    "    data_raw[0] = data_raw[0].to('cpu')  # .cuda()\n",
    "    data_raw[0] = util.crop_mult(data_raw[0], mult=8)\n",
    "    break\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8841a640-df4c-49f9-ad2b-69789d29f81e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# with no points\n",
    "for (pp, sample_p) in enumerate(sample_ps):\n",
    "    # img_path = [string.replace('%08d_%.3f' % (i, sample_p), '.', 'p')]\n",
    "    img_path = [('%08d_%.3f' % (i, sample_p)).replace('.', 'p')]\n",
    "    data = util.get_colorization_data(data_raw, opt, ab_thresh=0., p=sample_p)\n",
    "    break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b009ce05-3397-4ece-b8d0-0c3b59c82bbc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/john/dev/colorization-pytorch/util/util.py:327: UserWarning: torch.range is deprecated and will be removed in a future release because its behavior is inconsistent with Python's range builtin. Instead, use torch.arange, which produces values in [start, end).\n",
      "  a_range = torch.range(-opt.ab_max, opt.ab_max, step=opt.ab_quant).to(data_ab_quant.device)[None,:,None,None]\n"
     ]
    }
   ],
   "source": [
    "model.set_input(data)\n",
    "\n",
    "model.test(True)  # True means that losses will be computed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "fbf1aef8-3b76-4558-9548-d583a1afb652",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "visuals = util.get_subset_dict(model.get_current_visuals(), to_visualize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "fcc1f2ab-9ca8-4029-9221-087427b03974",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "psnrs = util.calculate_psnr_np(util.tensor2im(visuals['real']), util.tensor2im(visuals['fake_reg']))\n",
    "entrs = model.get_current_losses()['G_entr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a60abd8b-084f-438c-aca7-01513e9091bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "850411a2-9559-4f3f-805d-8d1585a742f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "model1\n",
      "------------------------------\n",
      "Shape of intput Tensor: torch.Size([1, 64, 256, 256]) --> 4194304\n",
      "Kernel size: 32\n",
      "Pooled output: torch.Size([1, 64, 8, 8]) --> 4096\n",
      "Interpolation output: torch.Size([1, 64, 12, 12]) --> 9216\n",
      "Final Number of Params: 9216\n",
      "\n",
      "\n",
      "model2\n",
      "------------------------------\n",
      "Shape of intput Tensor: torch.Size([1, 128, 128, 128]) --> 2097152\n",
      "Kernel size: 16\n",
      "Pooled output: torch.Size([1, 128, 8, 8]) --> 8192\n",
      "Interpolation output: torch.Size([1, 128, 9, 8]) --> 9216\n",
      "Final Number of Params: 9216\n",
      "\n",
      "\n",
      "model3\n",
      "------------------------------\n",
      "Shape of intput Tensor: torch.Size([1, 256, 64, 64]) --> 1048576\n",
      "Kernel size: 16\n",
      "Pooled output: torch.Size([1, 256, 4, 4]) --> 4096\n",
      "Interpolation output: torch.Size([1, 256, 6, 6]) --> 9216\n",
      "Final Number of Params: 9216\n",
      "\n",
      "\n",
      "model4\n",
      "------------------------------\n",
      "Shape of intput Tensor: torch.Size([1, 512, 32, 32]) --> 524288\n",
      "Kernel size: 8\n",
      "Pooled output: torch.Size([1, 512, 4, 4]) --> 8192\n",
      "Final Number of Params: 8192\n",
      "\n",
      "\n",
      "model5\n",
      "------------------------------\n",
      "Shape of intput Tensor: torch.Size([1, 512, 32, 32]) --> 524288\n",
      "Kernel size: 8\n",
      "Pooled output: torch.Size([1, 512, 4, 4]) --> 8192\n",
      "Final Number of Params: 8192\n"
     ]
    }
   ],
   "source": [
    "kernel_sizes = {\n",
    "    'model1': 32,\n",
    "    'model2': 16,\n",
    "    'model3': 16,\n",
    "    'model4': 8,\n",
    "    'model5': 8,\n",
    "}\n",
    "\n",
    "interpolate_size = {\n",
    "    'model1': (12, 12),\n",
    "    'model2': (9, 8),\n",
    "    'model3': (6, 6),\n",
    "    'model4': None,\n",
    "    'model5': None,\n",
    "}\n",
    "\n",
    "get_params = lambda tensor: reduce(lambda x, y: x*y, tensor.shape)\n",
    "\n",
    "for key, items in activation.items():\n",
    "    \n",
    "    print(f\"\\n\\n{key}\")\n",
    "    print(\"-\"*30)\n",
    "    \n",
    "    # Acquire dimensions\n",
    "    batch, depth, width, height = items[0].shape\n",
    "    print(f\"Shape of intput Tensor: {items[0].shape} --> {get_params(items[0])}\")\n",
    "    \n",
    "    # Get kernel size\n",
    "    kernel_size = kernel_sizes[key]\n",
    "    print(f\"Kernel size: {kernel_size}\")\n",
    "    \n",
    "    # Pool the tensor\n",
    "    output = F.avg_pool2d(items[0], kernel_size=kernel_size, stride=kernel_size)\n",
    "    print(f\"Pooled output: {output.shape} --> {get_params(output)}\")\n",
    "    \n",
    "    interp_size = interpolate_size[key]\n",
    "    if interp_size is not None:\n",
    "        output = F.interpolate(input=output, size=interp_size, scale_factor=None, \n",
    "                               mode='bilinear', align_corners=True, recompute_scale_factor=None)\n",
    "        print(f\"Interpolation output: {output.shape} --> {get_params(output)}\")\n",
    "        \n",
    "    final_output_params = get_params(output)\n",
    "    print(f\"Final Number of Params: {final_output_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "4d4182ed-d95b-484d-be3e-955701cdcaf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17.998046875"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "9215 / 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "49c6911d-e10e-4ac9-b47a-d768d278a74e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.333333333333333"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "32/6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0788e4c9-ed06-4ce5-8ca8-78f650c92ef4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "1be6583f-e4ce-4fa1-8259-64e3a45d3c72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "18 / 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "244d09d6-6a8d-4a0d-bd3d-15b2ec80b3f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 64, 12, 12])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.interpolate(input=output, size=(12, 12), scale_factor=None, mode='bilinear', align_corners=True, recompute_scale_factor=None).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2c75f63c-e804-4667-b063-93504151d659",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1, 2, 4, 8, 16, 32, 64, 128, 256}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from functools import reduce\n",
    "\n",
    "def factors(n):    \n",
    "    return set(reduce(list.__add__, \n",
    "                ([i, n//i] for i in range(1, int(n**0.5) + 1) if n % i == 0)))\n",
    "\n",
    "factors(256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ea95ecb1-27bf-443d-ae3d-70521dd71bde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.111111111111111"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "64 / 9\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ecc353f0-4741-44d8-ab6d-f55462d8c701",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32.0"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "64 / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4425aacb-303a-40a8-8e0d-acac18b752f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4096.0"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(64/16)**2 * 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ae5188a0-56c4-4b44-aa4d-8d89869bd588",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8192"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(32//8)**2 * 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8dd78fb2-d7b8-4be7-b3bb-a09621bd8947",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 256, 256])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "visuals['gray'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b892cab8-b808-4ac9-ab36-5d62ac33a40b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "im = util.tensor2im(visuals['gray'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "47c74bce-7422-422d-8358-7a576a1de5b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('uint8')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "im.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d5b0d77e-cf6b-4a7a-8e43-ec35bfd95337",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "Image.fromarray(im).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "85415763-65b3-4cc9-81fd-affe465f415e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'show'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_10829/1660183834.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'show'"
     ]
    }
   ],
   "source": [
    "im.show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9518097d-4abf-4213-89f4-d0f607d410f5",
   "metadata": {},
   "source": [
    "model.fc3.register_forward_hook(get_activation('fc3'))\n",
    "output = model(x)\n",
    "activation['fc3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e073580-7271-4761-af0f-1db698d0d0ee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# model = Pix2PixModel(*args, **kwargs)\n",
    "# model.load_state_dict(torch.load(PATH))\n",
    "# model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10cf5cdb-5572-497b-b25f-9a8270a82266",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
